# ğŸ§  Self-Driving Car using Q-Learning  

## ğŸ“Œ Overview  
This project demonstrates a simple **Reinforcement Learning (Q-Learning)** approach for teaching a car agent to navigate a grid environment safely, avoid obstacles, and reach its destination optimally.  

The project includes:  
- A **custom environment** simulating a small driving grid.  
- A **Q-Learning agent** that learns through trial and error.  
- **Training**, **testing**, and **performance analysis** modules.  
- A **documentation report** included for submission.  

---

## ğŸ—‚ï¸ Project Structure  

self_driving_qlearning/
â”‚
â”œâ”€â”€ drivingenv.py # Environment (grid, obstacles, reward logic)
â”œâ”€â”€ agent.py # Q-learning algorithm and agent behavior
â”œâ”€â”€ train_agent.py # Training and reward plotting
â”œâ”€â”€ test_agent.py # Testing trained agent accuracy
â”œâ”€â”€ performance_analysis.py # Reward trends and performance visualization
â”œâ”€â”€ training_rewards.pkl # Saved rewards from training
â”œâ”€â”€ report.docx / report.pdf # Final documentation report
â””â”€â”€ README.md # This file


---

## ğŸš— How It Works  

1. **Environment Setup (`driving_env.py`)**  
   - The car starts at `(0,0)` on a `5x5` grid.  
   - The goal is `(4,4)` and obstacles are placed in between.  
   - The agent receives:  
     - `+10` â†’ reaching goal  
     - `-10` â†’ hitting obstacle  
     - `-1` â†’ normal move  

2. **Agent (`q_learning_agent.py`)**  
   - Learns using **Q-Learning** with epsilon-greedy policy.  
   - Updates Q-values based on exploration and reward feedback.  

3. **Training (`train_agent.py`)**  
   - Trains for 1000 episodes.  
   - Tracks rewards and saves progress to `training_rewards.pkl`.  
   - Plots the **learning curve**.  

4. **Testing (`test_agent.py`)**  
   - Runs the trained model for multiple test episodes.  
   - Calculates success rate (accuracy).  
   - Displays reward distribution.  

5. **Performance Analysis (`performance_analysis.py`)**  
   - Loads reward data.  
   - Computes mean, max, min, and standard deviation.  
   - Plots moving-average learning trend.  

---

## âš™ï¸ Installation & Execution  

1ï¸âƒ£ Install dependencies  
```bash
pip install numpy matplotlib
2ï¸âƒ£ Train the agent
python train_agent.py
3ï¸âƒ£ Test the trained model
python test_agent.py
4ï¸âƒ£ Analyze performance
python performance_analysis.py

 ğŸ“ˆ Performance Analysis
     he trained agent is evaluated using two key metrics:
     
     1ï¸âƒ£ Trend Analysis
     hows how the **total reward** evolves over episodes, indicating the agentâ€™s learning      rogress.
     
     2ï¸âƒ£ Accuracy Evaluation
     alculates the **success rate** of the trained agent in reaching the goal.
     
     oth results are visualized through reward and accuracy plots.
     
---

ğŸ§¾ Documentation Report
A detailed report (report.docx / report.pdf) is included.
It covers:
Problem statement
Objective
Methodology (Q-Learning explanation)
Implementation snapshots
Results and graphs
Conclusion



ğŸ§  Key Learnings

How Q-Learning helps an agent make decisions based on rewards.
Exploration vs. exploitation balance.
Using reward trends to measure learning progress.
Visualization of reinforcement learning behavior.



ğŸ§© Future Enhancements
Add dynamic traffic or multiple moving obstacles.
Extend to continuous environments using Deep Q-Networks (DQN).
Integrate OpenAI Gym interface for benchmarking.



ğŸ‘©â€ğŸ’» Author
Mekha Sindhu

